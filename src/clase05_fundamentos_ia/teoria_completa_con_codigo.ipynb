{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  FUNDAMENTOS DE INTELIGENCIA ARTIFICIAL Y MACHINE LEARNING\n",
    "## Teor√≠a y Pr√°ctica con Python, Pandas y Matplotlib\n",
    "\n",
    "---\n",
    "\n",
    "###  Sobre este notebook\n",
    "\n",
    "Este notebook contiene **TODA la teor√≠a y pr√°ctica** necesaria para entender los fundamentos de IA y Machine Learning, aplicados al proyecto de Piedra, Papel o Tijera.\n",
    "\n",
    "**C√≥mo usar este notebook:**\n",
    "1. Lee cada secci√≥n de teor√≠a\n",
    "2. Ejecuta el c√≥digo para ver los conceptos en acci√≥n\n",
    "3. Completa los ejercicios marcados con X\n",
    "4. Experimenta modificando el c√≥digo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuraci√≥n inicial\n\n### Instalaci√≥n de dependencias (ejecutar solo si es necesario)\n\nSi alguna librer√≠a no est√° instalada, descomenta y ejecuta la siguiente celda:"
  },
  {
   "cell_type": "code",
   "source": "# Instalaci√≥n de dependencias para JupyterLite/Pyodide\nimport micropip\nawait micropip.install(['pandas', 'numpy', 'matplotlib', 'seaborn', 'scipy', 'scikit-learn'])\n\nprint(\"‚úÖ Todas las librer√≠as instaladas correctamente\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importar librer√≠as esenciales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "from scipy import stats\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Configurar pandas para mejor visualizaci√≥n\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Funci√≥n auxiliar para mostrar texto con formato\n",
    "def mostrar(texto, tipo='info'):\n",
    "    colores = {'info': '#3498db', 'success': '#2ecc71', 'warning': '#f39c12', 'danger': '#e74c3c'}\n",
    "    display(HTML(f'<div style=\"background-color:{colores[tipo]}20; padding:10px; border-left:4px solid {colores[tipo]}; border-radius:5px;\">{texto}</div>'))\n",
    "\n",
    "mostrar('‚úÖ <b>Librer√≠as cargadas correctamente</b><br>Pandas: ' + pd.__version__ + '<br>NumPy: ' + np.__version__, 'success')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  PARTE 1: ¬øQU√â ES LA INTELIGENCIA ARTIFICIAL?\n",
    "\n",
    "## 1.1 Definici√≥n y Conceptos B√°sicos\n",
    "\n",
    "La **Inteligencia Artificial (IA)** es la simulaci√≥n de procesos de inteligencia humana por parte de m√°quinas. En esencia, queremos que las m√°quinas:\n",
    "\n",
    "1. **APRENDAN** de los datos\n",
    "2. **RAZONEN** para resolver problemas\n",
    "3. **PERCIBAN** su entorno\n",
    "4. **ACT√öEN** para lograr objetivos\n",
    "\n",
    "### Diferencia entre Programaci√≥n Tradicional y Machine Learning:\n",
    "\n",
    "- **Programaci√≥n Tradicional**: `Reglas + Datos ‚Üí Respuesta`\n",
    "- **Machine Learning**: `Datos + Respuestas ‚Üí Reglas (Modelo)`\n",
    "\n",
    "Veamos esto con un ejemplo pr√°ctico:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EJEMPLO: Programaci√≥n Tradicional vs Machine Learning\n",
    "\n",
    "# ========== PROGRAMACI√ìN TRADICIONAL ==========\n",
    "def estrategia_tradicional(jugada_anterior):\n",
    "    \"\"\"Reglas fijas programadas manualmente\"\"\"\n",
    "    if jugada_anterior == 'piedra':\n",
    "        return 'papel'  # Regla fija: si jug√≥ piedra, juega papel\n",
    "    elif jugada_anterior == 'papel':\n",
    "        return 'tijera'\n",
    "    else:\n",
    "        return 'piedra'\n",
    "\n",
    "# ========== MACHINE LEARNING ==========\n",
    "class EstrategiaML:\n",
    "    def __init__(self):\n",
    "        self.datos = []  # Guardar√° el historial\n",
    "        self.modelo = {}  # Aprender√° patrones\n",
    "    \n",
    "    def entrenar(self, historial):\n",
    "        \"\"\"Aprende de los datos\"\"\"\n",
    "        # Contar qu√© sigue despu√©s de cada jugada\n",
    "        for i in range(len(historial)-1):\n",
    "            actual = historial[i]\n",
    "            siguiente = historial[i+1]\n",
    "            \n",
    "            if actual not in self.modelo:\n",
    "                self.modelo[actual] = Counter()\n",
    "            self.modelo[actual][siguiente] += 1\n",
    "    \n",
    "    def predecir(self, jugada_anterior):\n",
    "        \"\"\"Predice bas√°ndose en lo aprendido\"\"\"\n",
    "        if jugada_anterior in self.modelo:\n",
    "            # Devuelve la jugada m√°s probable\n",
    "            prediccion = self.modelo[jugada_anterior].most_common(1)[0][0]\n",
    "            # Devuelve lo que gana a la predicci√≥n\n",
    "            return {'piedra': 'papel', 'papel': 'tijera', 'tijera': 'piedra'}[prediccion]\n",
    "        return random.choice(['piedra', 'papel', 'tijera'])\n",
    "\n",
    "# Crear datos de ejemplo\n",
    "historial_ejemplo = ['piedra', 'tijera', 'piedra', 'papel', 'piedra', 'tijera', 'piedra', 'papel']\n",
    "\n",
    "# Probar ambos enfoques\n",
    "print(\"ü§ñ COMPARACI√ìN DE ENFOQUES:\\n\")\n",
    "print(\"Programaci√≥n Tradicional:\")\n",
    "print(f\"  Si el oponente jug√≥ 'piedra' ‚Üí Yo juego: {estrategia_tradicional('piedra')}\")\n",
    "print(f\"  (Siempre la misma respuesta)\\n\")\n",
    "\n",
    "print(\"Machine Learning:\")\n",
    "ml = EstrategiaML()\n",
    "ml.entrenar(historial_ejemplo)\n",
    "print(f\"  Modelo aprendido: {dict(ml.modelo)}\")\n",
    "print(f\"  Si el oponente jug√≥ 'piedra' ‚Üí Yo juego: {ml.predecir('piedra')}\")\n",
    "print(f\"  (Respuesta basada en patrones aprendidos)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  PARTE 2: FUNDAMENTOS DE PROBABILIDAD\n",
    "\n",
    "## 2.1 Probabilidad B√°sica\n",
    "\n",
    "La probabilidad es la base matem√°tica de toda la IA. Nos permite cuantificar la incertidumbre.\n",
    "\n",
    "### Definici√≥n:\n",
    "$$P(A) = \\frac{\\text{Casos favorables}}{\\text{Casos totales}}$$\n",
    "\n",
    "Donde:\n",
    "- $P(A)$ est√° entre 0 (imposible) y 1 (seguro)\n",
    "- En Piedra, Papel o Tijera: $P(\\text{cualquier jugada}) = \\frac{1}{3} ‚âà 0.333$ si es aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SIMULACI√ìN: Diferentes tipos de jugadores\n",
    "\n",
    "def simular_jugador(n=1000, tipo='aleatorio'):\n",
    "    \"\"\"Simula n jugadas de diferentes tipos de jugadores\"\"\"\n",
    "    opciones = ['piedra', 'papel', 'tijera']\n",
    "    \n",
    "    if tipo == 'aleatorio':\n",
    "        probs = [1/3, 1/3, 1/3]\n",
    "    elif tipo == 'agresivo':\n",
    "        probs = [0.6, 0.2, 0.2]  # Prefiere piedra\n",
    "    elif tipo == 'defensivo':\n",
    "        probs = [0.2, 0.6, 0.2]  # Prefiere papel\n",
    "    elif tipo == 't√°ctico':\n",
    "        probs = [0.2, 0.2, 0.6]  # Prefiere tijera\n",
    "    else:\n",
    "        probs = [1/3, 1/3, 1/3]\n",
    "    \n",
    "    return np.random.choice(opciones, size=n, p=probs)\n",
    "\n",
    "# Simular 1000 jugadas de cada tipo\n",
    "tipos_jugadores = ['aleatorio', 'agresivo', 'defensivo', 't√°ctico']\n",
    "datos_simulados = {}\n",
    "\n",
    "for tipo in tipos_jugadores:\n",
    "    datos_simulados[tipo] = simular_jugador(1000, tipo)\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "df_simulacion = pd.DataFrame(datos_simulados)\n",
    "\n",
    "# Mostrar primeras jugadas\n",
    "print(\"üéÆ PRIMERAS 10 JUGADAS DE CADA TIPO:\")\n",
    "display(df_simulacion.head(10))\n",
    "\n",
    "# Calcular probabilidades emp√≠ricas\n",
    "print(\"\\nüìä PROBABILIDADES EMP√çRICAS (basadas en 1000 jugadas):\")\n",
    "probabilidades = pd.DataFrame()\n",
    "\n",
    "for tipo in tipos_jugadores:\n",
    "    conteo = pd.Series(datos_simulados[tipo]).value_counts(normalize=True)\n",
    "    probabilidades[tipo] = conteo\n",
    "\n",
    "probabilidades = probabilidades.round(3).fillna(0)\n",
    "probabilidades['te√≥rico_aleatorio'] = 0.333\n",
    "\n",
    "# A√±adir estilo al DataFrame\n",
    "styled_df = probabilidades.style.background_gradient(cmap='YlOrRd', axis=1)\n",
    "display(styled_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# VISUALIZACI√ìN: Distribuci√≥n de probabilidades\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "colores = {'piedra': '#FF6B6B', 'papel': '#4ECDC4', 'tijera': '#45B7D1'}\n",
    "\n",
    "for idx, tipo in enumerate(tipos_jugadores):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Contar frecuencias\n",
    "    conteo = pd.Series(datos_simulados[tipo]).value_counts()\n",
    "    \n",
    "    # Crear gr√°fico de barras\n",
    "    bars = ax.bar(conteo.index, conteo.values, \n",
    "                   color=[colores[j] for j in conteo.index])\n",
    "    \n",
    "    # L√≠nea de referencia (distribuci√≥n uniforme)\n",
    "    ax.axhline(y=333, color='red', linestyle='--', alpha=0.5, \n",
    "               label='Esperado si aleatorio (333)')\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}\\n({height/10:.1f}%)',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    ax.set_title(f'Jugador {tipo.upper()}', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    ax.set_ylim(0, 700)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Distribuci√≥n de Jugadas por Tipo de Jugador', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Probabilidad Condicional\n",
    "\n",
    "La **probabilidad condicional** es crucial en IA. Nos dice: \"¬øCu√°l es la probabilidad de A, **dado que** B ya ocurri√≥?\"\n",
    "\n",
    "### Notaci√≥n:\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "### En nuestro juego:\n",
    "- $P(\\text{tijera} | \\text{gan√≥\\_√∫ltima})$ = Probabilidad de jugar tijera dado que gan√≥ la √∫ltima\n",
    "- $P(\\text{papel} | \\text{perdi√≥\\_dos\\_veces})$ = Probabilidad de papel tras perder dos veces"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EJEMPLO: Probabilidad condicional en acci√≥n\n",
    "\n",
    "# Generar datos con patrones condicionales\n",
    "def generar_jugadas_con_patron(n=500):\n",
    "    \"\"\"Genera jugadas donde el comportamiento depende del resultado anterior\"\"\"\n",
    "    jugadas = []\n",
    "    resultados = []\n",
    "    jugada_actual = random.choice(['piedra', 'papel', 'tijera'])\n",
    "    \n",
    "    for _ in range(n):\n",
    "        jugadas.append(jugada_actual)\n",
    "        \n",
    "        # Simular un resultado aleatorio\n",
    "        resultado = random.choice(['gana', 'pierde', 'empata'])\n",
    "        resultados.append(resultado)\n",
    "        \n",
    "        # El jugador cambia su estrategia seg√∫n el resultado\n",
    "        if resultado == 'gana':\n",
    "            # Si gana, tiende a repetir (60% probabilidad)\n",
    "            if random.random() < 0.6:\n",
    "                jugada_actual = jugada_actual\n",
    "            else:\n",
    "                jugada_actual = random.choice(['piedra', 'papel', 'tijera'])\n",
    "        elif resultado == 'pierde':\n",
    "            # Si pierde, cambia de estrategia (80% probabilidad)\n",
    "            if random.random() < 0.8:\n",
    "                opciones = ['piedra', 'papel', 'tijera']\n",
    "                opciones.remove(jugada_actual)\n",
    "                jugada_actual = random.choice(opciones)\n",
    "            else:\n",
    "                jugada_actual = jugada_actual\n",
    "        else:\n",
    "            # Si empata, comportamiento aleatorio\n",
    "            jugada_actual = random.choice(['piedra', 'papel', 'tijera'])\n",
    "    \n",
    "    return jugadas[:-1], resultados[:-1], jugadas[1:]  # actual, resultado, siguiente\n",
    "\n",
    "# Generar datos\n",
    "jugadas_actual, resultados, jugadas_siguiente = generar_jugadas_con_patron(1000)\n",
    "\n",
    "# Crear DataFrame para an√°lisis\n",
    "df_condicional = pd.DataFrame({\n",
    "    'jugada_actual': jugadas_actual,\n",
    "    'resultado': resultados,\n",
    "    'jugada_siguiente': jugadas_siguiente\n",
    "})\n",
    "\n",
    "print(\"üìä AN√ÅLISIS DE PROBABILIDAD CONDICIONAL\\n\")\n",
    "print(\"Muestra de datos:\")\n",
    "display(df_condicional.head(10))\n",
    "\n",
    "# Calcular probabilidades condicionales\n",
    "print(\"\\nüéØ PROBABILIDADES CONDICIONALES:\")\n",
    "print(\"P(jugada_siguiente | resultado)\\n\")\n",
    "\n",
    "# Crear tabla de probabilidades condicionales\n",
    "tabla_condicional = pd.crosstab(\n",
    "    df_condicional['resultado'], \n",
    "    df_condicional['jugada_siguiente'], \n",
    "    normalize='index'\n",
    ").round(3)\n",
    "\n",
    "# Visualizar con mapa de calor\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(tabla_condicional, annot=True, cmap='YlOrRd', \n",
    "            fmt='.3f', cbar_kws={'label': 'Probabilidad'})\n",
    "plt.title('Probabilidad de la Siguiente Jugada dado el Resultado Anterior', fontsize=14)\n",
    "plt.ylabel('Resultado Anterior')\n",
    "plt.xlabel('Siguiente Jugada')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis detallado\n",
    "print(\"\\nüí° INTERPRETACI√ìN:\")\n",
    "for resultado in tabla_condicional.index:\n",
    "    jugada_mas_probable = tabla_condicional.loc[resultado].idxmax()\n",
    "    prob = tabla_condicional.loc[resultado].max()\n",
    "    print(f\"Despu√©s de {resultado}: m√°s probable jugar {jugada_mas_probable} ({prob:.1%})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Teorema de Bayes\n",
    "\n",
    "El **Teorema de Bayes** es fundamental en Machine Learning. Nos permite actualizar nuestras creencias bas√°ndonos en nueva evidencia.\n",
    "\n",
    "### F√≥rmula:\n",
    "$$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$$\n",
    "\n",
    "Donde:\n",
    "- $P(A|B)$ = Probabilidad posterior (lo que queremos saber)\n",
    "- $P(B|A)$ = Verosimilitud \n",
    "- $P(A)$ = Probabilidad a priori\n",
    "- $P(B)$ = Evidencia"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EJEMPLO: Aplicando Bayes para predecir el tipo de jugador\n",
    "\n",
    "def aplicar_bayes(jugadas_observadas):\n",
    "    \"\"\"\n",
    "    Usa el teorema de Bayes para inferir qu√© tipo de jugador es\n",
    "    bas√°ndose en las jugadas observadas\n",
    "    \"\"\"\n",
    "    # Probabilidades a priori (asumimos igual probabilidad inicial)\n",
    "    prior = {\n",
    "        'aleatorio': 0.25,\n",
    "        'agresivo': 0.25,\n",
    "        'defensivo': 0.25,\n",
    "        't√°ctico': 0.25\n",
    "    }\n",
    "    \n",
    "    # Verosimilitudes (probabilidad de cada jugada dado el tipo)\n",
    "    verosimilitud = {\n",
    "        'aleatorio': {'piedra': 1/3, 'papel': 1/3, 'tijera': 1/3},\n",
    "        'agresivo': {'piedra': 0.6, 'papel': 0.2, 'tijera': 0.2},\n",
    "        'defensivo': {'piedra': 0.2, 'papel': 0.6, 'tijera': 0.2},\n",
    "        't√°ctico': {'piedra': 0.2, 'papel': 0.2, 'tijera': 0.6}\n",
    "    }\n",
    "    \n",
    "    # Calcular P(jugadas|tipo) para cada tipo\n",
    "    probabilidades = {}\n",
    "    \n",
    "    for tipo in prior:\n",
    "        # P(jugadas|tipo) = producto de las probabilidades individuales\n",
    "        prob = prior[tipo]\n",
    "        for jugada in jugadas_observadas:\n",
    "            prob *= verosimilitud[tipo][jugada]\n",
    "        probabilidades[tipo] = prob\n",
    "    \n",
    "    # Normalizar (para que sumen 1)\n",
    "    total = sum(probabilidades.values())\n",
    "    for tipo in probabilidades:\n",
    "        probabilidades[tipo] /= total\n",
    "    \n",
    "    return probabilidades\n",
    "\n",
    "# Observar jugadas de un jugador desconocido\n",
    "print(\"üé≤ INFERENCIA BAYESIANA: ¬øQu√© tipo de jugador es?\\n\")\n",
    "\n",
    "# Caso 1: Jugador que juega mucha piedra\n",
    "jugadas_caso1 = ['piedra', 'piedra', 'papel', 'piedra', 'piedra', 'tijera', 'piedra']\n",
    "resultado1 = aplicar_bayes(jugadas_caso1)\n",
    "\n",
    "print(f\"Caso 1 - Jugadas observadas: {jugadas_caso1}\")\n",
    "df_bayes1 = pd.DataFrame([resultado1]).T\n",
    "df_bayes1.columns = ['Probabilidad']\n",
    "df_bayes1 = df_bayes1.sort_values('Probabilidad', ascending=False)\n",
    "print(\"\\nProbabilidad de cada tipo:\")\n",
    "display(df_bayes1.style.bar(color='lightblue'))\n",
    "\n",
    "# Caso 2: Jugador equilibrado\n",
    "jugadas_caso2 = ['piedra', 'papel', 'tijera', 'papel', 'piedra', 'tijera']\n",
    "resultado2 = aplicar_bayes(jugadas_caso2)\n",
    "\n",
    "print(f\"\\nCaso 2 - Jugadas observadas: {jugadas_caso2}\")\n",
    "df_bayes2 = pd.DataFrame([resultado2]).T\n",
    "df_bayes2.columns = ['Probabilidad']\n",
    "df_bayes2 = df_bayes2.sort_values('Probabilidad', ascending=False)\n",
    "print(\"\\nProbabilidad de cada tipo:\")\n",
    "display(df_bayes2.style.bar(color='lightgreen'))\n",
    "\n",
    "# Visualizaci√≥n comparativa\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Caso 1\n",
    "axes[0].bar(df_bayes1.index, df_bayes1['Probabilidad'], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#95E77E'])\n",
    "axes[0].set_title('Caso 1: Muchas piedras', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Probabilidad')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Caso 2\n",
    "axes[1].bar(df_bayes2.index, df_bayes2['Probabilidad'], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#95E77E'])\n",
    "axes[1].set_title('Caso 2: Equilibrado', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Probabilidad')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Inferencia Bayesiana del Tipo de Jugador', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  PARTE 3: ENTROP√çA Y TEOR√çA DE LA INFORMACI√ìN\n",
    "\n",
    "## 3.1 Entrop√≠a de Shannon\n",
    "\n",
    "La **entrop√≠a** mide el desorden o incertidumbre en los datos. En nuestro contexto:\n",
    "- **Entrop√≠a alta** = Jugador impredecible (bueno)\n",
    "- **Entrop√≠a baja** = Jugador predecible (malo)\n",
    "\n",
    "### F√≥rmula:\n",
    "$$H(X) = -\\sum_{i} P(x_i) \\times \\log_2(P(x_i))$$\n",
    "\n",
    "Para 3 opciones equiprobables: $H_{max} = \\log_2(3) ‚âà 1.585$ bits"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# IMPLEMENTACI√ìN: C√°lculo de entrop√≠a\n",
    "\n",
    "def calcular_entropia(jugadas):\n",
    "    \"\"\"Calcula la entrop√≠a de Shannon de una secuencia de jugadas\"\"\"\n",
    "    # Contar frecuencias\n",
    "    conteo = pd.Series(jugadas).value_counts()\n",
    "    total = len(jugadas)\n",
    "    \n",
    "    # Calcular probabilidades\n",
    "    probabilidades = conteo / total\n",
    "    \n",
    "    # Calcular entrop√≠a\n",
    "    entropia = 0\n",
    "    for p in probabilidades:\n",
    "        if p > 0:\n",
    "            entropia -= p * np.log2(p)\n",
    "    \n",
    "    return entropia\n",
    "\n",
    "# Crear diferentes secuencias para comparar\n",
    "secuencias = {\n",
    "    'Totalmente predecible': ['piedra'] * 100,\n",
    "    'Muy predecible': ['piedra'] * 80 + ['papel'] * 15 + ['tijera'] * 5,\n",
    "    'Algo predecible': ['piedra'] * 50 + ['papel'] * 30 + ['tijera'] * 20,\n",
    "    'Casi aleatorio': ['piedra'] * 35 + ['papel'] * 33 + ['tijera'] * 32,\n",
    "    'Perfectamente aleatorio': np.random.choice(['piedra', 'papel', 'tijera'], 100)\n",
    "}\n",
    "\n",
    "# Calcular entrop√≠as\n",
    "resultados_entropia = {}\n",
    "for nombre, secuencia in secuencias.items():\n",
    "    entropia = calcular_entropia(secuencia)\n",
    "    resultados_entropia[nombre] = {\n",
    "        'Entrop√≠a': entropia,\n",
    "        '% del m√°ximo': (entropia / np.log2(3)) * 100\n",
    "    }\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "df_entropia = pd.DataFrame(resultados_entropia).T\n",
    "df_entropia = df_entropia.sort_values('Entrop√≠a')\n",
    "\n",
    "print(\"üìä AN√ÅLISIS DE ENTROP√çA\\n\")\n",
    "print(f\"Entrop√≠a m√°xima te√≥rica (3 opciones): {np.log2(3):.3f} bits\\n\")\n",
    "display(df_entropia.style.background_gradient(subset=['Entrop√≠a'], cmap='RdYlGn'))\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gr√°fico de barras de entrop√≠a\n",
    "ax1 = axes[0]\n",
    "bars = ax1.barh(df_entropia.index, df_entropia['Entrop√≠a'], \n",
    "                 color=plt.cm.RdYlGn(df_entropia['Entrop√≠a']/np.log2(3)))\n",
    "ax1.axvline(x=np.log2(3), color='red', linestyle='--', alpha=0.5, label='M√°ximo te√≥rico')\n",
    "ax1.set_xlabel('Entrop√≠a (bits)')\n",
    "ax1.set_title('Entrop√≠a por Tipo de Secuencia', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for bar, (idx, row) in zip(bars, df_entropia.iterrows()):\n",
    "    ax1.text(row['Entrop√≠a'] + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "             f\"{row['Entrop√≠a']:.3f}\", va='center')\n",
    "\n",
    "# Distribuciones de ejemplo\n",
    "ax2 = axes[1]\n",
    "ejemplos = ['Totalmente predecible', 'Algo predecible', 'Perfectamente aleatorio']\n",
    "x = np.arange(3)\n",
    "width = 0.25\n",
    "\n",
    "for i, ejemplo in enumerate(ejemplos):\n",
    "    conteo = pd.Series(secuencias[ejemplo]).value_counts()\n",
    "    valores = [conteo.get('piedra', 0), conteo.get('papel', 0), conteo.get('tijera', 0)]\n",
    "    ax2.bar(x + i*width, valores, width, label=ejemplo)\n",
    "\n",
    "ax2.set_xlabel('Jugada')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "ax2.set_title('Distribuci√≥n de Jugadas por Tipo', fontweight='bold')\n",
    "ax2.set_xticks(x + width)\n",
    "ax2.set_xticklabels(['Piedra', 'Papel', 'Tijera'])\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mostrar('üí° <b>Interpretaci√≥n:</b> A mayor entrop√≠a, m√°s dif√≠cil es predecir al jugador. Un jugador con entrop√≠a m√°xima (1.585) es completamente impredecible.', 'info')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  PARTE 4: TIPOS DE APRENDIZAJE AUTOM√ÅTICO\n",
    "\n",
    "## 4.1 Aprendizaje Supervisado\n",
    "\n",
    "En el **aprendizaje supervisado**, tenemos datos etiquetados (sabemos la respuesta correcta) y el modelo aprende la relaci√≥n entrada‚Üísalida.\n",
    "\n",
    "**Ejemplo en PPT**: Dado el historial `[piedra, papel]` ‚Üí etiqueta: `tijera` (lo que jug√≥ despu√©s)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EJEMPLO: Aprendizaje Supervisado simple\n",
    "\n",
    "class AprendizajeSupervisado:\n",
    "    def __init__(self, ventana=2):\n",
    "        self.ventana = ventana\n",
    "        self.modelo = {}\n",
    "        \n",
    "    def crear_dataset(self, historial):\n",
    "        \"\"\"Crea dataset de entrenamiento con ventanas de tama√±o fijo\"\"\"\n",
    "        X = []  # Features (ventanas de jugadas)\n",
    "        y = []  # Labels (siguiente jugada)\n",
    "        \n",
    "        for i in range(len(historial) - self.ventana):\n",
    "            patron = tuple(historial[i:i+self.ventana])\n",
    "            siguiente = historial[i+self.ventana]\n",
    "            X.append(patron)\n",
    "            y.append(siguiente)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def entrenar(self, X, y):\n",
    "        \"\"\"Entrena el modelo con los datos etiquetados\"\"\"\n",
    "        # Para cada patr√≥n, guardar qu√© jugada sigue m√°s frecuentemente\n",
    "        for patron, etiqueta in zip(X, y):\n",
    "            if patron not in self.modelo:\n",
    "                self.modelo[patron] = Counter()\n",
    "            self.modelo[patron][etiqueta] += 1\n",
    "    \n",
    "    def predecir(self, patron):\n",
    "        \"\"\"Predice la siguiente jugada bas√°ndose en el patr√≥n\"\"\"\n",
    "        patron_tupla = tuple(patron)\n",
    "        if patron_tupla in self.modelo:\n",
    "            return self.modelo[patron_tupla].most_common(1)[0][0]\n",
    "        return random.choice(['piedra', 'papel', 'tijera'])\n",
    "    \n",
    "    def evaluar(self, X_test, y_test):\n",
    "        \"\"\"Eval√∫a la precisi√≥n del modelo\"\"\"\n",
    "        correctos = 0\n",
    "        for patron, etiqueta_real in zip(X_test, y_test):\n",
    "            prediccion = self.predecir(patron)\n",
    "            if prediccion == etiqueta_real:\n",
    "                correctos += 1\n",
    "        return correctos / len(y_test) if len(y_test) > 0 else 0\n",
    "\n",
    "# Generar datos con patr√≥n claro\n",
    "print(\"ü§ñ APRENDIZAJE SUPERVISADO EN ACCI√ìN\\n\")\n",
    "\n",
    "# Crear secuencia con patr√≥n: despu√©s de [piedra, papel] siempre viene tijera\n",
    "historial_patron = []\n",
    "patrones_definidos = {\n",
    "    ('piedra', 'papel'): 'tijera',\n",
    "    ('papel', 'tijera'): 'piedra',\n",
    "    ('tijera', 'piedra'): 'papel',\n",
    "    ('piedra', 'piedra'): 'papel',\n",
    "    ('papel', 'papel'): 'tijera',\n",
    "    ('tijera', 'tijera'): 'piedra'\n",
    "}\n",
    "\n",
    "# Generar historial siguiendo los patrones\n",
    "historial_patron = ['piedra', 'papel']  # Inicio\n",
    "for _ in range(100):\n",
    "    ultimo_patron = tuple(historial_patron[-2:])\n",
    "    if ultimo_patron in patrones_definidos:\n",
    "        siguiente = patrones_definidos[ultimo_patron]\n",
    "    else:\n",
    "        siguiente = random.choice(['piedra', 'papel', 'tijera'])\n",
    "    historial_patron.append(siguiente)\n",
    "\n",
    "# Crear y entrenar modelo\n",
    "modelo = AprendizajeSupervisado(ventana=2)\n",
    "\n",
    "# Dividir en entrenamiento (70%) y prueba (30%)\n",
    "split_point = int(len(historial_patron) * 0.7)\n",
    "train_data = historial_patron[:split_point]\n",
    "test_data = historial_patron[split_point:]\n",
    "\n",
    "# Crear datasets\n",
    "X_train, y_train = modelo.crear_dataset(train_data)\n",
    "X_test, y_test = modelo.crear_dataset(test_data)\n",
    "\n",
    "print(f\"üìä Dataset de entrenamiento: {len(X_train)} ejemplos\")\n",
    "print(f\"üìä Dataset de prueba: {len(X_test)} ejemplos\\n\")\n",
    "\n",
    "# Entrenar\n",
    "modelo.entrenar(X_train, y_train)\n",
    "\n",
    "# Evaluar\n",
    "precision = modelo.evaluar(X_test, y_test)\n",
    "\n",
    "print(f\"‚úÖ Precisi√≥n en datos de prueba: {precision:.2%}\\n\")\n",
    "\n",
    "# Mostrar lo que aprendi√≥ el modelo\n",
    "print(\"üß† MODELO APRENDIDO:\")\n",
    "df_modelo = pd.DataFrame([\n",
    "    {\n",
    "        'Patr√≥n': str(patron),\n",
    "        'Predicci√≥n': contador.most_common(1)[0][0],\n",
    "        'Confianza': contador.most_common(1)[0][1] / sum(contador.values())\n",
    "    }\n",
    "    for patron, contador in sorted(modelo.modelo.items())[:10]\n",
    "])\n",
    "\n",
    "display(df_modelo.style.background_gradient(subset=['Confianza'], cmap='Greens'))\n",
    "\n",
    "# Visualizar predicciones vs realidad\n",
    "predicciones = [modelo.predecir(x) for x in X_test[:20]]\n",
    "realidad = y_test[:20]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# Mapear a n√∫meros para visualizar\n",
    "mapeo = {'piedra': 0, 'papel': 1, 'tijera': 2}\n",
    "pred_num = [mapeo[p] for p in predicciones]\n",
    "real_num = [mapeo[r] for r in realidad]\n",
    "\n",
    "x = range(len(predicciones))\n",
    "ax.plot(x, real_num, 'o-', label='Real', markersize=8, linewidth=2)\n",
    "ax.plot(x, pred_num, 's--', label='Predicci√≥n', markersize=6, alpha=0.7, linewidth=2)\n",
    "\n",
    "# Marcar aciertos y errores\n",
    "for i in range(len(predicciones)):\n",
    "    if pred_num[i] == real_num[i]:\n",
    "        ax.plot(i, pred_num[i], 'g*', markersize=15, alpha=0.5)\n",
    "    else:\n",
    "        ax.plot(i, pred_num[i], 'rx', markersize=10, linewidth=3)\n",
    "\n",
    "ax.set_yticks([0, 1, 2])\n",
    "ax.set_yticklabels(['Piedra', 'Papel', 'Tijera'])\n",
    "ax.set_xlabel('Ejemplo de prueba')\n",
    "ax.set_title('Predicciones vs Realidad (‚òÖ=acierto, ‚úó=error)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Aprendizaje No Supervisado\n",
    "\n",
    "En el **aprendizaje no supervisado**, no tenemos etiquetas. El modelo debe encontrar estructura o patrones por s√≠ mismo.\n",
    "\n",
    "**Aplicaci√≥n en PPT**: Agrupar jugadores por estilo sin saber de antemano qu√© tipos existen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EJEMPLO: Clustering de jugadores por estilo\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generar datos de m√∫ltiples jugadores\n",
    "def crear_perfil_jugador(jugadas):\n",
    "    \"\"\"Crea un vector de caracter√≠sticas para un jugador\"\"\"\n",
    "    total = len(jugadas)\n",
    "    if total == 0:\n",
    "        return [0] * 6\n",
    "    \n",
    "    conteo = pd.Series(jugadas).value_counts()\n",
    "    \n",
    "    # Caracter√≠sticas\n",
    "    features = [\n",
    "        conteo.get('piedra', 0) / total,  # % piedra\n",
    "        conteo.get('papel', 0) / total,   # % papel\n",
    "        conteo.get('tijera', 0) / total,  # % tijera\n",
    "        calcular_entropia(jugadas),        # Entrop√≠a\n",
    "        len(set(jugadas)) / 3,             # Diversidad (0-1)\n",
    "    ]\n",
    "    \n",
    "    # Calcular rachas (jugadas consecutivas iguales)\n",
    "    rachas = 1\n",
    "    max_racha = 1\n",
    "    for i in range(1, len(jugadas)):\n",
    "        if jugadas[i] == jugadas[i-1]:\n",
    "            rachas += 1\n",
    "            max_racha = max(max_racha, rachas)\n",
    "        else:\n",
    "            rachas = 1\n",
    "    features.append(max_racha / total)  # Tendencia a repetir\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Simular 50 jugadores diferentes\n",
    "print(\"üßÆ APRENDIZAJE NO SUPERVISADO: CLUSTERING DE JUGADORES\\n\")\n",
    "\n",
    "jugadores_data = []\n",
    "for i in range(50):\n",
    "    # Crear jugadores con diferentes estilos\n",
    "    tipo_random = np.random.choice(['aleatorio', 'agresivo', 'defensivo', 't√°ctico', 'cambiante'])\n",
    "    \n",
    "    if tipo_random == 'cambiante':\n",
    "        # Jugador que cambia de estilo\n",
    "        jugadas = list(simular_jugador(50, 'agresivo')) + list(simular_jugador(50, 'defensivo'))\n",
    "    else:\n",
    "        jugadas = simular_jugador(100, tipo_random)\n",
    "    \n",
    "    perfil = crear_perfil_jugador(jugadas)\n",
    "    jugadores_data.append(perfil)\n",
    "\n",
    "# Crear DataFrame\n",
    "columnas = ['% Piedra', '% Papel', '% Tijera', 'Entrop√≠a', 'Diversidad', 'Tendencia Repetir']\n",
    "df_jugadores = pd.DataFrame(jugadores_data, columns=columnas)\n",
    "\n",
    "print(\"Muestra de perfiles de jugadores:\")\n",
    "display(df_jugadores.head(10).round(3))\n",
    "\n",
    "# Normalizar datos\n",
    "scaler = StandardScaler()\n",
    "datos_normalizados = scaler.fit_transform(df_jugadores)\n",
    "\n",
    "# Aplicar K-Means\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "clusters = kmeans.fit_predict(datos_normalizados)\n",
    "\n",
    "# A√±adir clusters al DataFrame\n",
    "df_jugadores['Cluster'] = clusters\n",
    "\n",
    "# Analizar clusters\n",
    "print(\"\\nüìä AN√ÅLISIS DE CLUSTERS:\")\n",
    "resumen_clusters = df_jugadores.groupby('Cluster').mean().round(3)\n",
    "display(resumen_clusters.style.background_gradient(cmap='coolwarm', axis=0))\n",
    "\n",
    "# Interpretaci√≥n de clusters\n",
    "print(\"\\nüí° INTERPRETACI√ìN DE CLUSTERS:\")\n",
    "for cluster in range(4):\n",
    "    perfil = resumen_clusters.loc[cluster]\n",
    "    \n",
    "    # Determinar tipo bas√°ndose en caracter√≠sticas\n",
    "    if perfil['% Piedra'] > 0.45:\n",
    "        tipo = \"AGRESIVO (prefiere piedra)\"\n",
    "    elif perfil['% Papel'] > 0.45:\n",
    "        tipo = \"DEFENSIVO (prefiere papel)\"\n",
    "    elif perfil['% Tijera'] > 0.45:\n",
    "        tipo = \"T√ÅCTICO (prefiere tijera)\"\n",
    "    elif perfil['Entrop√≠a'] > 1.5:\n",
    "        tipo = \"ALEATORIO (alta entrop√≠a)\"\n",
    "    else:\n",
    "        tipo = \"MIXTO\"\n",
    "    \n",
    "    print(f\"Cluster {cluster}: {tipo}\")\n",
    "    print(f\"  - Entrop√≠a promedio: {perfil['Entrop√≠a']:.3f}\")\n",
    "    print(f\"  - Jugada dominante: {['Piedra', 'Papel', 'Tijera'][np.argmax([perfil['% Piedra'], perfil['% Papel'], perfil['% Tijera']])]}\")\n",
    "    print()\n",
    "\n",
    "# Visualizaci√≥n de clusters\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reducir a 2 dimensiones para visualizar\n",
    "pca = PCA(n_components=2)\n",
    "datos_2d = pca.fit_transform(datos_normalizados)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(datos_2d[:, 0], datos_2d[:, 1], c=clusters, \n",
    "                      cmap='viridis', s=100, alpha=0.6, edgecolors='black')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# A√±adir centroides\n",
    "centroides_2d = pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centroides_2d[:, 0], centroides_2d[:, 1], \n",
    "            marker='*', s=300, c='red', edgecolors='black', linewidth=2,\n",
    "            label='Centroides')\n",
    "\n",
    "plt.xlabel(f'Componente Principal 1 ({pca.explained_variance_ratio_[0]:.1%} varianza)')\n",
    "plt.ylabel(f'Componente Principal 2 ({pca.explained_variance_ratio_[1]:.1%} varianza)')\n",
    "plt.title('Clustering de Jugadores por Estilo de Juego', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Aprendizaje por Refuerzo\n",
    "\n",
    "En el **aprendizaje por refuerzo**, un agente aprende mediante prueba y error, recibiendo recompensas o penalizaciones.\n",
    "\n",
    "**Componentes clave**:\n",
    "- **Estado**: Situaci√≥n actual\n",
    "- **Acci√≥n**: Qu√© hacer\n",
    "- **Recompensa**: Feedback (+1 ganar, -1 perder, 0 empatar)\n",
    "- **Pol√≠tica**: Estrategia aprendida"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EJEMPLO: Q-Learning simplificado para PPT\n",
    "\n",
    "class QLearningPPT:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        alpha: tasa de aprendizaje\n",
    "        gamma: factor de descuento\n",
    "        epsilon: exploraci√≥n vs explotaci√≥n\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Tabla Q: estado -> acci√≥n -> valor\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        self.acciones = ['piedra', 'papel', 'tijera']\n",
    "        \n",
    "    def obtener_estado(self, ultimas_jugadas):\n",
    "        \"\"\"Convierte las √∫ltimas jugadas en un estado\"\"\"\n",
    "        return tuple(ultimas_jugadas[-3:]) if len(ultimas_jugadas) >= 3 else tuple(ultimas_jugadas)\n",
    "    \n",
    "    def elegir_accion(self, estado):\n",
    "        \"\"\"Elige acci√≥n usando estrategia epsilon-greedy\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            # Exploraci√≥n: acci√≥n aleatoria\n",
    "            return random.choice(self.acciones)\n",
    "        else:\n",
    "            # Explotaci√≥n: mejor acci√≥n seg√∫n Q-table\n",
    "            valores_q = self.q_table[estado]\n",
    "            if not valores_q:\n",
    "                return random.choice(self.acciones)\n",
    "            \n",
    "            max_valor = max(valores_q.values())\n",
    "            mejores_acciones = [a for a, v in valores_q.items() if v == max_valor]\n",
    "            return random.choice(mejores_acciones)\n",
    "    \n",
    "    def calcular_recompensa(self, mi_jugada, jugada_oponente):\n",
    "        \"\"\"Calcula la recompensa seg√∫n el resultado\"\"\"\n",
    "        if mi_jugada == jugada_oponente:\n",
    "            return 0  # Empate\n",
    "        elif (mi_jugada == 'piedra' and jugada_oponente == 'tijera') or \\\n",
    "             (mi_jugada == 'papel' and jugada_oponente == 'piedra') or \\\n",
    "             (mi_jugada == 'tijera' and jugada_oponente == 'papel'):\n",
    "            return 1  # Victoria\n",
    "        else:\n",
    "            return -1  # Derrota\n",
    "    \n",
    "    def actualizar_q(self, estado, accion, recompensa, nuevo_estado):\n",
    "        \"\"\"Actualiza el valor Q usando la ecuaci√≥n de Bellman\"\"\"\n",
    "        q_actual = self.q_table[estado][accion]\n",
    "        \n",
    "        # Mejor valor Q del nuevo estado\n",
    "        if nuevo_estado in self.q_table and self.q_table[nuevo_estado]:\n",
    "            max_q_nuevo = max(self.q_table[nuevo_estado].values())\n",
    "        else:\n",
    "            max_q_nuevo = 0\n",
    "        \n",
    "        # Ecuaci√≥n de actualizaci√≥n Q-learning\n",
    "        nuevo_q = q_actual + self.alpha * (recompensa + self.gamma * max_q_nuevo - q_actual)\n",
    "        self.q_table[estado][accion] = nuevo_q\n",
    "    \n",
    "    def entrenar(self, num_episodios=1000):\n",
    "        \"\"\"Entrena el agente jugando contra un oponente\"\"\"\n",
    "        historial_recompensas = []\n",
    "        \n",
    "        for episodio in range(num_episodios):\n",
    "            historial_oponente = []\n",
    "            recompensa_total = 0\n",
    "            \n",
    "            # Jugar 20 rondas por episodio\n",
    "            for _ in range(20):\n",
    "                # Estado actual\n",
    "                estado = self.obtener_estado(historial_oponente)\n",
    "                \n",
    "                # Elegir acci√≥n\n",
    "                mi_jugada = self.elegir_accion(estado)\n",
    "                \n",
    "                # Oponente juega (puede ser cualquier estrategia)\n",
    "                if len(historial_oponente) > 0 and random.random() < 0.7:\n",
    "                    # 70% repite su √∫ltima jugada (oponente predecible)\n",
    "                    jugada_oponente = historial_oponente[-1]\n",
    "                else:\n",
    "                    jugada_oponente = random.choice(self.acciones)\n",
    "                \n",
    "                # Calcular recompensa\n",
    "                recompensa = self.calcular_recompensa(mi_jugada, jugada_oponente)\n",
    "                recompensa_total += recompensa\n",
    "                \n",
    "                # Actualizar historial\n",
    "                historial_oponente.append(jugada_oponente)\n",
    "                \n",
    "                # Nuevo estado\n",
    "                nuevo_estado = self.obtener_estado(historial_oponente)\n",
    "                \n",
    "                # Actualizar Q-table\n",
    "                self.actualizar_q(estado, mi_jugada, recompensa, nuevo_estado)\n",
    "            \n",
    "            historial_recompensas.append(recompensa_total)\n",
    "            \n",
    "            # Reducir exploraci√≥n gradualmente\n",
    "            if episodio % 100 == 0:\n",
    "                self.epsilon *= 0.95\n",
    "        \n",
    "        return historial_recompensas\n",
    "\n",
    "# Entrenar agente Q-Learning\n",
    "print(\"üéÆ APRENDIZAJE POR REFUERZO: Q-LEARNING\\n\")\n",
    "\n",
    "agente = QLearningPPT(alpha=0.1, gamma=0.9, epsilon=0.3)\n",
    "recompensas = agente.entrenar(num_episodios=500)\n",
    "\n",
    "# Analizar evoluci√≥n del aprendizaje\n",
    "ventana = 20\n",
    "recompensas_suavizadas = pd.Series(recompensas).rolling(ventana).mean()\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Evoluci√≥n de recompensas\n",
    "ax1 = axes[0]\n",
    "ax1.plot(recompensas, alpha=0.3, label='Recompensa por episodio')\n",
    "ax1.plot(recompensas_suavizadas, linewidth=2, label=f'Media m√≥vil ({ventana} episodios)')\n",
    "ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Break-even')\n",
    "ax1.set_xlabel('Episodio')\n",
    "ax1.set_ylabel('Recompensa total')\n",
    "ax1.set_title('Evoluci√≥n del Aprendizaje', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribuci√≥n de recompensas\n",
    "ax2 = axes[1]\n",
    "primeros_100 = recompensas[:100]\n",
    "ultimos_100 = recompensas[-100:]\n",
    "\n",
    "ax2.hist(primeros_100, bins=20, alpha=0.5, label='Primeros 100 episodios', color='red')\n",
    "ax2.hist(ultimos_100, bins=20, alpha=0.5, label='√öltimos 100 episodios', color='green')\n",
    "ax2.axvline(x=np.mean(primeros_100), color='red', linestyle='--', label=f'Media inicial: {np.mean(primeros_100):.1f}')\n",
    "ax2.axvline(x=np.mean(ultimos_100), color='green', linestyle='--', label=f'Media final: {np.mean(ultimos_100):.1f}')\n",
    "ax2.set_xlabel('Recompensa total por episodio')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "ax2.set_title('Comparaci√≥n: Inicio vs Final del Entrenamiento', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar mejora\n",
    "mejora = (np.mean(ultimos_100) - np.mean(primeros_100)) / abs(np.mean(primeros_100)) * 100 if np.mean(primeros_100) != 0 else 0\n",
    "mostrar(f'üìà <b>Mejora del agente:</b> {mejora:.1f}% de incremento en recompensa promedio<br>' + \n",
    "        f'Recompensa inicial promedio: {np.mean(primeros_100):.2f}<br>' +\n",
    "        f'Recompensa final promedio: {np.mean(ultimos_100):.2f}', 'success')\n",
    "\n",
    "# Mostrar algunas entradas de la Q-table\n",
    "print(\"\\nüß† MUESTRA DE LA Q-TABLE APRENDIDA:\")\n",
    "q_sample = pd.DataFrame([\n",
    "    {\n",
    "        'Estado': str(estado),\n",
    "        'Mejor Acci√≥n': max(acciones.items(), key=lambda x: x[1])[0],\n",
    "        'Valor Q': max(acciones.values())\n",
    "    }\n",
    "    for estado, acciones in list(agente.q_table.items())[:10]\n",
    "    if acciones\n",
    "])\n",
    "display(q_sample)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìà PARTE 5: CADENAS DE MARKOV Y PREDICCI√ìN\n",
    "\n",
    "## 5.1 Cadenas de Markov\n",
    "\n",
    "Una **Cadena de Markov** modela transiciones entre estados, donde la probabilidad del siguiente estado depende solo del estado actual (propiedad de Markov).\n",
    "\n",
    "### Matriz de Transici√≥n:\n",
    "$$P(X_{t+1} = j | X_t = i) = P_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# IMPLEMENTACI√ìN: Cadenas de Markov para PPT\n",
    "\n",
    "class CadenaMarkovPPT:\n",
    "    def __init__(self):\n",
    "        self.estados = ['piedra', 'papel', 'tijera']\n",
    "        self.matriz_transicion = pd.DataFrame(\n",
    "            np.zeros((3, 3)),\n",
    "            index=self.estados,\n",
    "            columns=self.estados\n",
    "        )\n",
    "        self.conteo_transiciones = pd.DataFrame(\n",
    "            np.zeros((3, 3)),\n",
    "            index=self.estados,\n",
    "            columns=self.estados\n",
    "        )\n",
    "    \n",
    "    def entrenar(self, secuencia):\n",
    "        \"\"\"Construye la matriz de transici√≥n a partir de una secuencia\"\"\"\n",
    "        # Contar transiciones\n",
    "        for i in range(len(secuencia) - 1):\n",
    "            estado_actual = secuencia[i]\n",
    "            estado_siguiente = secuencia[i + 1]\n",
    "            self.conteo_transiciones.loc[estado_actual, estado_siguiente] += 1\n",
    "        \n",
    "        # Normalizar para obtener probabilidades\n",
    "        for estado in self.estados:\n",
    "            total = self.conteo_transiciones.loc[estado].sum()\n",
    "            if total > 0:\n",
    "                self.matriz_transicion.loc[estado] = self.conteo_transiciones.loc[estado] / total\n",
    "    \n",
    "    def predecir(self, estado_actual):\n",
    "        \"\"\"Predice el siguiente estado bas√°ndose en la matriz de transici√≥n\"\"\"\n",
    "        probabilidades = self.matriz_transicion.loc[estado_actual]\n",
    "        \n",
    "        if probabilidades.sum() == 0:\n",
    "            return random.choice(self.estados)\n",
    "        \n",
    "        # Elegir seg√∫n las probabilidades\n",
    "        return np.random.choice(self.estados, p=probabilidades)\n",
    "    \n",
    "    def predecir_determinista(self, estado_actual):\n",
    "        \"\"\"Predice el estado m√°s probable (sin aleatoriedad)\"\"\"\n",
    "        probabilidades = self.matriz_transicion.loc[estado_actual]\n",
    "        \n",
    "        if probabilidades.sum() == 0:\n",
    "            return random.choice(self.estados)\n",
    "        \n",
    "        return probabilidades.idxmax()\n",
    "\n",
    "# Crear y entrenar modelo de Markov\n",
    "print(\"‚õìÔ∏è CADENAS DE MARKOV PARA PREDICCI√ìN\\n\")\n",
    "\n",
    "# Generar secuencia con patrones de transici√≥n claros\n",
    "secuencia_markov = []\n",
    "estado = 'piedra'\n",
    "secuencia_markov.append(estado)\n",
    "\n",
    "# Definir probabilidades de transici√≥n\n",
    "transiciones_reales = {\n",
    "    'piedra': {'piedra': 0.2, 'papel': 0.5, 'tijera': 0.3},\n",
    "    'papel': {'piedra': 0.4, 'papel': 0.1, 'tijera': 0.5},\n",
    "    'tijera': {'piedra': 0.6, 'papel': 0.3, 'tijera': 0.1}\n",
    "}\n",
    "\n",
    "for _ in range(999):\n",
    "    probs = transiciones_reales[estado]\n",
    "    estado = np.random.choice(list(probs.keys()), p=list(probs.values()))\n",
    "    secuencia_markov.append(estado)\n",
    "\n",
    "# Entrenar modelo\n",
    "modelo_markov = CadenaMarkovPPT()\n",
    "modelo_markov.entrenar(secuencia_markov)\n",
    "\n",
    "# Mostrar matriz de transici√≥n aprendida\n",
    "print(\"üìä MATRIZ DE TRANSICI√ìN APRENDIDA:\")\n",
    "print(\"(Probabilidad de ir de fila a columna)\\n\")\n",
    "styled_matrix = modelo_markov.matriz_transicion.style.background_gradient(cmap='YlOrRd', axis=1).format(\"{:.3f}\")\n",
    "display(styled_matrix)\n",
    "\n",
    "# Comparar con la matriz real\n",
    "print(\"\\nüìä MATRIZ DE TRANSICI√ìN REAL (para comparaci√≥n):\")\n",
    "matriz_real = pd.DataFrame(transiciones_reales).T\n",
    "display(matriz_real.style.background_gradient(cmap='YlOrRd', axis=1).format(\"{:.3f}\"))\n",
    "\n",
    "# Visualizaci√≥n de la matriz de transici√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap de la matriz aprendida\n",
    "sns.heatmap(modelo_markov.matriz_transicion, annot=True, fmt='.3f', \n",
    "            cmap='YlOrRd', cbar_kws={'label': 'Probabilidad'},\n",
    "            ax=axes[0], vmin=0, vmax=1)\n",
    "axes[0].set_title('Matriz de Transici√≥n Aprendida', fontweight='bold')\n",
    "axes[0].set_xlabel('Estado Siguiente')\n",
    "axes[0].set_ylabel('Estado Actual')\n",
    "\n",
    "# Diagrama de red\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(-1.5, 1.5)\n",
    "ax2.set_ylim(-1.5, 1.5)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "# Posiciones de los nodos\n",
    "pos = {\n",
    "    'piedra': (0, 1),\n",
    "    'papel': (-0.866, -0.5),\n",
    "    'tijera': (0.866, -0.5)\n",
    "}\n",
    "\n",
    "# Dibujar nodos\n",
    "for estado, (x, y) in pos.items():\n",
    "    circle = plt.Circle((x, y), 0.3, color='lightblue', ec='black', linewidth=2)\n",
    "    ax2.add_patch(circle)\n",
    "    ax2.text(x, y, estado, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Dibujar transiciones principales (solo las m√°s probables)\n",
    "for origen in modelo_markov.estados:\n",
    "    for destino in modelo_markov.estados:\n",
    "        prob = modelo_markov.matriz_transicion.loc[origen, destino]\n",
    "        if prob > 0.3:  # Solo mostrar transiciones significativas\n",
    "            x1, y1 = pos[origen]\n",
    "            x2, y2 = pos[destino]\n",
    "            \n",
    "            if origen == destino:\n",
    "                # Auto-loop\n",
    "                ax2.add_patch(plt.Circle((x1, y1 + 0.4), 0.15, \n",
    "                                        fill=False, ec='red', \n",
    "                                        linewidth=prob*5))\n",
    "            else:\n",
    "                # Flecha entre estados\n",
    "                ax2.annotate('', xy=(x2*0.7, y2*0.7), xytext=(x1*0.7, y1*0.7),\n",
    "                           arrowprops=dict(arrowstyle='->', lw=prob*5, \n",
    "                                         color='red', alpha=0.7))\n",
    "                # Etiqueta con probabilidad\n",
    "                mx, my = (x1*0.7 + x2*0.7)/2, (y1*0.7 + y2*0.7)/2\n",
    "                ax2.text(mx, my, f'{prob:.2f}', fontsize=9, \n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "ax2.set_title('Diagrama de Transiciones (P > 0.3)', fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Predicci√≥n usando el modelo\n",
    "print(\"\\nüîÆ PREDICCIONES USANDO EL MODELO:\")\n",
    "estado_test = 'piedra'\n",
    "print(f\"Estado actual: {estado_test}\")\n",
    "print(f\"Predicci√≥n m√°s probable: {modelo_markov.predecir_determinista(estado_test)}\")\n",
    "print(f\"\\nSimulaci√≥n de pr√≥ximas 5 jugadas:\")\n",
    "for i in range(5):\n",
    "    siguiente = modelo_markov.predecir(estado_test)\n",
    "    print(f\"  {i+1}. {estado_test} ‚Üí {siguiente}\")\n",
    "    estado_test = siguiente"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ PARTE 6: EJERCICIOS PR√ÅCTICOS\n",
    "\n",
    "Ahora es tu turno de aplicar lo aprendido. Completa los siguientes ejercicios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Ejercicio 1: An√°lisis de Probabilidades\n",
    "\n",
    "Dado el siguiente historial de jugadas, calcula:\n",
    "1. La probabilidad de cada jugada\n",
    "2. La entrop√≠a del jugador\n",
    "3. ¬øEs predecible o aleatorio?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EJERCICIO 1: Completa el c√≥digo\n",
    "\n",
    "historial_ejercicio = ['piedra', 'piedra', 'papel', 'piedra', 'tijera', \n",
    "                       'piedra', 'piedra', 'papel', 'piedra', 'piedra',\n",
    "                       'tijera', 'piedra', 'papel', 'piedra', 'piedra']\n",
    "\n",
    "# TODO: Calcula las probabilidades\n",
    "# probabilidades = ...\n",
    "\n",
    "# TODO: Calcula la entrop√≠a\n",
    "# entropia = ...\n",
    "\n",
    "# TODO: Determina si es predecible (entrop√≠a < 1.0) o aleatorio (entrop√≠a > 1.4)\n",
    "# tipo_jugador = ...\n",
    "\n",
    "# Descomenta para ver la soluci√≥n:\n",
    "# mostrar('Pista: Usa pd.Series(historial).value_counts(normalize=True) para las probabilidades', 'info')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Ejercicio 2: Implementa tu Predictor\n",
    "\n",
    "Crea un predictor que combine frecuencia y patrones:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EJERCICIO 2: Implementa tu propio predictor\n",
    "\n",
    "class MiPredictor:\n",
    "    def __init__(self):\n",
    "        self.historial = []\n",
    "        # TODO: A√±ade las estructuras de datos que necesites\n",
    "        pass\n",
    "    \n",
    "    def actualizar(self, jugada):\n",
    "        \"\"\"Actualiza el historial con la nueva jugada\"\"\"\n",
    "        self.historial.append(jugada)\n",
    "        # TODO: Actualiza cualquier otra estructura necesaria\n",
    "        pass\n",
    "    \n",
    "    def predecir(self):\n",
    "        \"\"\"Predice la pr√≥xima jugada del oponente\"\"\"\n",
    "        # TODO: Implementa tu l√≥gica de predicci√≥n\n",
    "        # Combina frecuencia y patrones\n",
    "        # Devuelve la jugada que GANA a tu predicci√≥n\n",
    "        \n",
    "        return random.choice(['piedra', 'papel', 'tijera'])  # Reemplaza esto\n",
    "\n",
    "# Prueba tu predictor\n",
    "mi_predictor = MiPredictor()\n",
    "test_sequence = ['piedra', 'papel', 'tijera', 'piedra', 'papel']\n",
    "for jugada in test_sequence:\n",
    "    mi_predictor.actualizar(jugada)\n",
    "\n",
    "print(f\"Mi predicci√≥n: {mi_predictor.predecir()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Ejercicio 3: An√°lisis Bayesiano\n",
    "\n",
    "Observas que un jugador ha hecho: [tijera, tijera, papel, tijera]\n",
    "\n",
    "¬øQu√© tipo de jugador es m√°s probable que sea?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EJERCICIO 3: Inferencia Bayesiana\n",
    "\n",
    "observaciones = ['tijera', 'tijera', 'papel', 'tijera']\n",
    "\n",
    "# TODO: Usa el teorema de Bayes para determinar el tipo m√°s probable\n",
    "# Considera los tipos: aleatorio, agresivo, defensivo, t√°ctico\n",
    "# con las probabilidades definidas anteriormente\n",
    "\n",
    "# tipo_mas_probable = ...\n",
    "# probabilidad = ...\n",
    "\n",
    "# print(f\"Tipo m√°s probable: {tipo_mas_probable} con probabilidad {probabilidad:.2%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üèÜ CONCLUSIONES Y PR√ìXIMOS PASOS\n",
    "\n",
    "## Lo que hemos aprendido:\n",
    "\n",
    "1. **Fundamentos de IA**: Diferencia entre programaci√≥n tradicional y ML\n",
    "2. **Probabilidad y Estad√≠stica**: Base matem√°tica para predicciones\n",
    "3. **Entrop√≠a**: Medida de incertidumbre y aleatoriedad\n",
    "4. **Tipos de Aprendizaje**: Supervisado, No Supervisado, Por Refuerzo\n",
    "5. **Algoritmos**: Frecuencias, Patrones, Markov, Q-Learning\n",
    "6. **Implementaci√≥n**: C√≥digo Python con pandas y visualizaciones\n",
    "\n",
    "## Para tu proyecto de Piedra, Papel o Tijera:\n",
    "\n",
    "### Nivel B√°sico (5-6 puntos):\n",
    "- Implementa predictor de frecuencias\n",
    "- Guarda y analiza datos con pandas\n",
    "- Calcula m√©tricas b√°sicas\n",
    "\n",
    "### Nivel Intermedio (7-8 puntos):\n",
    "- A√±ade predictor de patrones\n",
    "- Implementa Cadenas de Markov\n",
    "- Visualiza resultados con matplotlib\n",
    "\n",
    "### Nivel Avanzado (9-10 puntos):\n",
    "- Combina m√∫ltiples predictores\n",
    "- Implementa aprendizaje adaptativo\n",
    "- A√±ade an√°lisis de entrop√≠a y Bayes\n",
    "\n",
    "## Recursos adicionales:\n",
    "\n",
    "- **Documentaci√≥n Pandas**: https://pandas.pydata.org/docs/\n",
    "- **Matplotlib Gallery**: https://matplotlib.org/stable/gallery/\n",
    "- **Scikit-learn**: https://scikit-learn.org/stable/\n",
    "- **Teor√≠a de Juegos**: https://en.wikipedia.org/wiki/Game_theory\n",
    "\n",
    "## üéÆ ¬°Ahora est√°s listo para crear tu IA!\n",
    "\n",
    "Recuerda:\n",
    "- La IA perfecta no existe (teorema minimax)\n",
    "- Pero puedes acercarte mucho\n",
    "- Lo importante es entender C√ìMO y POR QU√â funciona\n",
    "\n",
    "### ¬°Que gane el mejor algoritmo! üèÜ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# BONUS: Guarda este notebook con tus soluciones\n",
    "mostrar('üéâ <b>¬°Felicidades!</b><br>Has completado el tutorial de fundamentos de IA.<br>Ahora aplica estos conceptos en tu proyecto.', 'success')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
